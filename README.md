# L1-Divergence
A Python Script to Compare L1 Divergence to KL Divergence. This trains a teacher model, and subtracts the absolute value of the student's generated probabilities from the probabilities the teacher generates from Cross Entropy + Softmax . It also throws in L2 .
